containers are isolated environments that allow you to run your code independently of the current environment you are running in 

dockers help with creating and managing containers

containers are better than virtual machines in the sense that they don't need other operating systems to run , all they need is just the container support on the os and then docker handles their creation and management
, they are more lightweight , faster and take less resources compared to a classic vm 

docker run -p 8000:3000 imageName : this will run the image and the -p will Publish the port 3000 that is exposed on the docker container to the port 8000 of the local system 

docker ps : lists all running containers

docker stop containerName : this will stop a running container

images are the blue prints for containers 

containers : are the running instances of the images , images are the static config of the containers 

docker run nonExistingImage : this will search for the image offline and if it's not found it will install it from docker hub 

docker ps -a : is the abbreviation of docker Processes where -a will show you all the processes that are currently running 

docker run -it image : this will expose the running image inside the container so we can interact with it 

FROM node15 : this allows u to build your image on top of another image which is node in this case  and it caches the base image 

COPY . . : this first dot refers to the root directory of local machine and the second dot refers to the working directory on the image container  , and the command copies the first path to the second one 
example:
COPY . /app : this will copy the root directory of the local machine to a created folder on the image container called app

RUN npm install : the RUN command will execute a command on the docker container

WORKDIR /app : by default the root folder of the container is the working directory or is the directory where all your commands are going to be executed , here we change the work directory to /app 

RUN : is executed when the image is being built

CMD ['node','server.js']: is executed when the container is being built and in this case it will start the server

EXPOSE 80 : this will expose port 80 from the container "this is for documentation purposes and an optional condition but it should be added"

images are static and are not changed when you change your source code 

docker images layered which means that every command in the docker file is executed in a separate layer , and if it's not changed it will be used from cache 
if a command is changed in the docker file , then all subsequent layers will be rebuilt from scratch 

COPY package.json /app
RUN npm install 
COPY . /app : this separation  is better cause now package.json will only be invalidated from the cache if package.json was changed instead of always reinstalling packages if anything in the source code was changed

docker --help : shows you the list of commands that are available for use 

docker run imageName : this will create a new container and run it and it runs in the foreground , so you can't
enter more commands 

docker start containerName : this will start a container that was exited , this is better if your source code hasn't 
changed and it starts the container in the background 

docker run -d imageName : this will run an image in detached mode , allowing you to run the image in the background

docker attach containerName : this will attach the current console to the container and allow you to see 
the output of the current running container 

docker logs containerName : this allows you to have the logs of the currently running container

docker start -a containerName : this allows you to start a container in attached mode 

docker run -i -t imageName || docker run -it imageName : this allows you to run an image in interactive mode thanks to the -i flag and exposes
a terminal for you using the -t to interact with the running container thanks to the -t flag 

docker start -a -i containerName : this allows you to start a container in attached interactive mode so you can 
input to the running container 

docker rm containerName : removes a container 

docker container prune : removes all stopped containers at once  

docker image prune : removes all unused images

docker image prune -a :  removes all images even if used 

docker images : shows you all the images on your machine 

docker rmi imageName : this allows you to remove images 

docker run --rm imageName : the rm flag removes the container once it's exited

docker image inspect ImageName : gives you more data about the image like the date it was created on and the operating
system of the image 

docker cp dummy/. containerName:/folder : this will copy data from your local host to your running container , here
it will copy all the dummy folder cause of the /. and it will copy it to the container folder , always in the first 
part you put the source and in the second part you put the destination 

docker run --name containerName imageName: this will run the container and give it the name you specify which in this
case is containerName

name:tag : is how you can name images , where name represents the image group and tag specifies the specific image
we are targeting , and the tag is optional 

docker build -t name:tag .  : this will build the image and give it a name of name and a tag of tag, and tag is 
optional 

sharing an image is done by sharing the docker file and the source code 

docker login : this will allow you to enter your docker hub credentials before pushing to the server 

docker push imageName : this is going to upload your image to a repository management service like docker hub
and the repo name has to be the same as the image name and you need to run docker login before pushing

docker tag oldName newName : this will rename an image 

docker pull imageName : this will allow you to pull an image from a repository 

anonymous volumes will be destroyed on removal of containers and the data will be lost 

named volumes will stay after the containers have be destroyed 

bind mounts will have their changes reflected immediately when they are done 

read only mounts : those are volumes that can only read the data on the host machine but can't edit it 

docker volume --help : this will show you the list of the volumes that docker is managing

docker volume ls : this shows you all the volumes that you have 

docker volume create volumeName : this will create a volume with the specified name 

docker volume rm volumeName : removes the specified volume

docker volume prune : removes all volumes

docker volume inspect volumeName : this will show you all the details about a specific volume , like the day it was created and so on

.dockerignore : is a docker file that specifies which files inside your project should be ignored when executing a copy instruction inside your dockerFile

usually having node_modules in the .dockerignore is a good idea cause it makes building faster as node_modules folder is installed inside the docker using npm install
also having the DockerFile file and and .git is a good practice as they are needed inside the image or the container 

ARG someValue : this allows you to set argument values inside your DockerFile instead of having to add --arg in your build command 

ENV PORT 80 : ENV variables are variables that will be available for your entire project code and you could use them in nodejs by accessing process.env , and in this case we are setting the
environment variable port to value equal 80

EXPOSE $PORT : this will allow you to access the environment variable port , using the $ sign

docker run --env PORT=1000 imageName || docker run -e PORT=100 imageName : this will set the environment variable port to 1000 when starting the container 

docker run --env-file ./.env : this will reference the .env file in your project and make the env variables there available during the running of the container , the ./ is for referencing the current
folder that the command is run in 

it's more secure to have your environment variables at a separate file rather than in the dockerFile , as the docker file env variables are baked into the image and anyone can then view you 
env variables by doing docker history imageName

ARGs are BUILD time variables that couldn't be used in your code 

ARG DEFAULT_PORT=80
ENV PORT $DEFAULT_PORT : this here will allow you to use your default port value to be equal with the port environment variable which is available during run time 

docker build --build-arg DEFAULT_PORT=8000 : will overwrite the DEFAULT_PORT value in the dockerFile and so the environment variable PORT will be equal to 8000 instead of 80

networks requests in docker containers work outside of the of the box with no need for special config 

if you want to connect to a running server on your machine , you need to change the localhost part of the connection string and this is how :
instead of for example 'mongodb://localhost:27017//databaseName' , it needs to be 'mongodb://host.docker.internal:27017//databaseName' 
host.docker.internal : is a syntax only understood by docker containers at run time and will resolve to the ip of the hosting machine of the container

docker run mongo : since the image of mongodb isn't installed , this command will download the image from docker hub and spin up it's server 

network : is a connection between containers that allow them to communicate if they are on the same network

docker network create networkName : this allows you to create a docker network 

docker network ls : shows you a list of docker networks 

docker run --name imageName : this will run a container of the image and give it the specified image name 

if two containers are running on the same network , they can connect to each other by their container names 
example : 
if we have a mongodb container named mongo and nodejs app on the same network , then the connection string on the mongo app will be the following
'mongodb://mongo:27017/dbName'

docker run --network networkName : this allows the docker image to run and be on the specified network

docker doesn't replace the hostname in you source code, it simply tracks the network requests coming out of your containers and is able to resolve the docker specific domains like 
host.docker.internal and resolve them to the required service 

docker network create --driver driverName networkName : this will create a network that uses the driverName as it's default driver , the default driver is the 'bridge' driver which works in 
most cases 

frontend end applications run in the browser and can't access server dependencies like the backend or database container through container names

docker run -v data:/data/db : this is a named volume , and it allows volumes to survive container removal 

docker compose purpose , is to make building and managing containers easier , so you don't have to write all the docker commands to start your environment

docker compose services are terminology for the running containers 

docker-compose.yaml : is the docker compose file where you write your container configuration to run multiple containers at the same time

version: "3.8" : this is the first line you write in a docker compose file , it defines the version of docker that is going to be used for the dockerfile 

yaml is like python in that it uses indentation to store values and everything indented by two spaces is a child of the parent element

services:
  container1:
  container2:
  container3:
this is how you define containers in a docker compose file and in each container you define it's configuration

container:
  image: 
this is how you define the image that the container is going to use example
mongodb:
  image: 'mongo'
  volumes: 
    - data:/data/db
  environment:
    ENV_NAME: value
  env_file:
    ./file.env
this is going to download mongo image and build the container on top of it , and it has a volume called data that will be stored in the path data/db
the container will also have an env variable of name env_name and value of value , you can also write environment variables like this 
- ENV= value
env_file syntax will allow loading variables from a specific file in your project

dashes or - are added before values that don't have a key pair , as an object is created with the files directly from the yaml file 

docker compose file will add all services on the same network by default 

volumes:
  data: 
this is how you create named volumes , if there was a named volume specified in the services , you have to put it here like this so that docker registers it 

docker-compose up , will build all the images required automatically without needing docker-compose building

docker-compose down -v: removes all images and containers that are a result of docker-compose up and -v removes all volumes

service:
  build: ./dockerfile
this allows you to use the docker file in the specified location and build the image based on it 

service:
  build:
    context: ./path
    dockerfile: DockerFile Changed name
this alternative syntax is used in case the docker file name is changed we use the docker file option , in case we want to provide a more broad context than that is in the docker file we use 
the context option meaning if the docker file is located in a nested folder for example and the dockerfile is accessing the parent folder , we need to set the parent path to the context option

service:
 args:
  ARG:value
this is how you could specify arguments for your service instead of the ARG keyword in your dockerfile

service:
  volumes:
    - namedPath:/path
this how you add a named volume under a service and then you add it to the volumes top level key like in the volumes: data example above

service:
  volumes:
    - ./backend:/app
this is how you set a bind mount on the backend folder , you specify the path and the path of main folder of the app inside the container, bind mounts are not added to the volumes top level key
only named volumes gets added 

service:
  depends_on:
    - anotherServiceName

the depends on option forces docker to initialize the dependencies of a service before launching itself

you can use docker service names and reference them inside your code 

service:
  ports:
    - '3000:80'
this is how you set exposed ports in you app , where 3000 is the external port and 80 is the internal docker port 


when you stop a running docker-compose config , it takes down all the services and removes them 

service: 
  volumes:
    - ./backend:/app
this will create a bind mount , where all the changes happening in the backend folder in the previous example will be reflected without the need for restarting containers

service: 
  volumes:
    - /app/node_modules
this is an anonymous volumes that doesn't need to be named or added into the volumes key in docker-compose config


service:
  stdin_open: true
  tty: true
this will launch this service in interactive mode which is the command '-it' in docker run commands , the tty allows for an open terminal and stdin_open allows the service to take input

docker-compose up --build : the --build forces the images to be rebuilt

docker-compose build : only builds images without running them

npm init : allows you to initialize a package.json file

docker exec -it node npm init : the exec command allows you to execute commands in running containers , for example here we will execute npm init in running node environment , and the it will allow
us to have the container running in interactive mode

docker run -it -v /absolutePathOfYourFolder:/pathInsideContainer node npm init : this will run a node image in interactive mode and have bind mount to the app folder on the container , and execute
npm init , the command format is like this 'docker run -v bindMount imageName command

if we add a command with docker run like the previous example , it will override the command or CMD specified in the dockerfile

ENTRYPOINT : a dockerfile command that allows you to have an initial command and then every command you execute after the run command will be added after it
like the following ENTRYPOINT ['npm'] then doing the previous example command your new command will simple be
docker run -it -v bindMount imageName init , instead of having npm before the init

docker-compose run service : docker-compose run allows you to execute a specific service out of your docker-compose file

docker-compose run service command : this allows you to execute a command after the service

utility containers: a term by max , that allows you to run containers that aren't meant to be used for applications , instead , they are only used for running commands that only could be run 
with specific dependencies that aren't installed on your machine

docker-compose up service1 service2 .... : you can bring up multiple services using 'docker-compose up' followed by the services you want to start

docker-compose up doesn't pick up changes you have done to your code , to rebuild your images add --build so your images are rebuilt for you

settings you put in your docker-compose file overrides your dockerfile settings

service:
  entrypoint: ['something']
this will add an entry point to your code and override the one in the dockerfile

service:
  working_dir: 'path'
this will set the working dir of the service and override the one in dockerfile

you don't have the run or cmd docker commands in the docker-compose file and it's only available in the dockerfile 

bind mounts are a bad idea in production cause every container should have everything it needs inside of it 

build:
  context: path
context doesn't just set the place where docker will search for dockerfiles , it also sets the place where the image will be built , so if you want to specify a specific path for your dockerfile
without changing the context you could do this
build:
 context: .
 dockerfile: pathToYourDockerfile


 the COPY dockerfile command takes a snapshot of whatever you choose to copy unlike bind mounts where your changes are reflected on the image

 one of the key advantages of docker is that your remote production environment can run your application as your local machine

 bind mounts shouldn't be used in production

 when you deploy a docker image to the cloud it's preferable to have an image pushed to the docker hub and then pulling it on the cloud machine rather than copying your source code on the cloud
 and creating the docker image to the machine

 *.env : this is what you add to your .gitignore to remove .env file from git 

 docker tag oldName newName : this will rename an image

 docker push imageName : allows you to push your image to docker hub but the image name should be the one specified on the docker repo , so you should rename your image using the previous command

 by default ec2 instances are protected from the world wide web , so you need to modify the security group's 'inbound rules' and allow the connection

 amazon ecs : amazon elastic container service which helps with the monitoring and management of instances instead of manually creating ec2 instances

 when you start the amazon ecs service and open the configuration , you are basically configuring the run command properties

 amazon ecs fargate : allows you to have a lambda like instances , where the instance of amazon will only start when a request is made to it and then shut down once the request is handled

 clusters: are collections of one or more containers that are able to talk to each other

 amazon auto scaling : scales your fargate instance based on the number of requests that it's accepting

 when you push changes to your docker image , you need to create a new 'task on ecs' so that ecs applies the changes

 when you build an image , the environment variables are fed to the instance of that image 

 task : is the configuration of the run command of your docker images

 the load balancer by default checks if the service is healthy by making a request to a certain url and then if it fails it restarts the instance

 efs : elastic file system is what ecs instances use to create volumes

 multistage dockerfile : allows you to have multiple stages for your project , by having multiple FROMs in your dockerfile, as every FROM will create a new stage and discard the previous one 

 FROM node as build : this as syntax will allow this stage to have a name that you can reference later in other stages 

 COPY --from=build /app/build /user/something : this command will take the build folder from the previous build stage and then copy it to the specified destination directory which is /user/something
 in this case

 containers are by nature stateless meaning that one container writing data will lose it when it's shut down or deleted unless there are volumes 

 services : are running tasks that was previously configured
most cases

the difference between a clusterIP service type and loadBalancer service type , is that although both of them have traffic balancing , 
clusterIP doesn't have out world facing ip address while loadBalancer has one 

coreDNS : allows for definitions of internal ips of your cluster

aws EKS : elastic kubernetes cluster , is a service that allows you to deploy your clusters easily
